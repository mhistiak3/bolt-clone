# Copy this file to .env and configure Ollama settings
# Ollama URL (default: http://localhost:11434)
OLLAMA_URL=http://localhost:11434
# Ollama model to use (default: codellama)
# You can also use: llama2, mistral, deepseek-coder, etc.
OLLAMA_MODEL=qwen2.5-coder:7b
